{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import spike_data_augmentation\n",
    "from spike_data_augmentation.datasets.dataloader import Dataloader\n",
    "import spike_data_augmentation.transforms as transforms\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import ipdb\n",
    "import numpy as np\n",
    "from utils.helper import plot_centers\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose dataset and representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "surface_dimensions = (11,11)\n",
    "dropout_probability = 0.5\n",
    "time_constant = 5e3\n",
    "n_of_centers = 25\n",
    "\n",
    "transform = transforms.Compose([transforms.DropEvent(drop_probability=dropout_probability)])\n",
    "representation = spike_data_augmentation.representations.Timesurface(surface_dimensions=surface_dimensions, tau=time_constant, merge_polarities=True)\n",
    "\n",
    "testset = spike_data_augmentation.datasets.NMNIST(save_to='./data', train=False, transform=transform, representation=representation, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read timesurfaces and use minibatch clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = Dataloader(testset, shuffle=True)\n",
    "testiterator = iter(testloader)\n",
    "\n",
    "batch_size = 100\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_of_centers, batch_size=batch_size)\n",
    "\n",
    "for surfaces, label in tqdm(testiterator):\n",
    "    surfaces = surfaces.reshape(-1, np.prod(surface_dimensions))\n",
    "    split = len(surfaces) // batch_size\n",
    "    surfs = np.array_split(surfaces, split)\n",
    "    for surf in surfs:\n",
    "        kmeans.partial_fit(surf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_.reshape((-1,) + surface_dimensions)\n",
    "activations = kmeans.counts_\n",
    "\n",
    "plot_centers(centers, activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build histograms for each datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = Dataloader(testset, shuffle=True)\n",
    "testiterator = iter(testloader)\n",
    "\n",
    "all_kmeans_labels = []\n",
    "all_labels = []\n",
    "for surfaces, label in tqdm(testiterator):\n",
    "    surfaces = surfaces.reshape(-1, np.prod(surface_dimensions))\n",
    "    surf_labels = kmeans.predict(surfaces)\n",
    "    all_kmeans_labels.append(surf_labels)\n",
    "    all_labels.append(label)\n",
    "    \n",
    "hists = []\n",
    "[hists.append(np.histogram(x, bins=np.arange(0,n_of_centers+1))[0]) for x in all_kmeans_labels]\n",
    "print('Histogram of first data point: ' + str(hists[0]))\n",
    "\n",
    "assert len(hists) == len(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(hists,all_labels,test_size=0.25, shuffle=True)\n",
    "training = collections.Counter(y_train)\n",
    "testing = collections.Counter(y_test)\n",
    "print('Training: ' + str(training)) \n",
    "print('Testing:  ' + str(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "logreg = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=10000)\n",
    "logreg.fit(X_train,y_train)\n",
    "#print(logreg.score(X_test, y_test))\n",
    "print(classification_report(y_test, logreg.predict(X_test)))\n",
    "print(confusion_matrix(y_test, logreg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
