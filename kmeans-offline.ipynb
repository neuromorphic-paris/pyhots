{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import spike_data_augmentation\n",
    "from spike_data_augmentation.datasets.dataloader import Dataloader\n",
    "from spike_data_augmentation import datasets\n",
    "import spike_data_augmentation.transforms as transforms\n",
    "import ipdb\n",
    "import numpy as np\n",
    "from utils.helper import plot_centers, create_histograms\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from time import gmtime, strftime\n",
    "print(strftime(\"Started on %a, %d %b %Y %H:%M:%S\", gmtime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose dataset and representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_centers = 100\n",
    "surface_dimensions = (11,11)\n",
    "transform = transforms.Compose([transforms.ToTimesurface(surface_dimensions=surface_dimensions, tau=5e3, merge_polarities=True)])\n",
    "\n",
    "testset = spike_data_augmentation.datasets.NMNIST(save_to='./data', transform=transform, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all timesurfaces and associated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = Dataloader(testset, shuffle=True)\n",
    "testiterator = iter(testloader)\n",
    "\n",
    "all_surfaces = []\n",
    "all_labels = []\n",
    "for surfaces, label in tqdm(testiterator):\n",
    "    surfaces = surfaces.reshape((-1, *surface_dimensions))\n",
    "    all_surfaces.append(surfaces)\n",
    "    all_labels.append(label)\n",
    "stack = np.vstack(all_surfaces)\n",
    "print('Read ' + str(stack.shape[0]) + ' surfaces.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_of_centers).fit(stack)\n",
    "centers = kmeans.cluster_centers_.reshape((-1,)+surface_dimensions)\n",
    "activations = kmeans.counts_\n",
    "\n",
    "plot_centers(centers, activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build histograms for each datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_indices = []\n",
    "running_i = 0\n",
    "for x in all_surfaces:\n",
    "    running_i += len(x)\n",
    "    recording_indices.append(running_i)\n",
    "del recording_indices[-1]\n",
    "\n",
    "# build histograms from split kmean labels\n",
    "kmeans_split = np.split(kmeans.labels_, recording_indices)\n",
    "hists = []\n",
    "[hists.append(np.histogram(x, bins=np.arange(0,n_of_centers+1))[0]/len(x)) for x in kmeans_split]\n",
    "print('Histogram of first data point: ' + str(hists[0]))\n",
    "\n",
    "assert np.sum(hists[-1]) == len(all_surfaces[-1])\n",
    "assert len(hists) == len(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(hists,all_labels,test_size=0.25, shuffle=True)\n",
    "training = collections.Counter(y_train)\n",
    "testing = collections.Counter(y_test)\n",
    "print('Training: ' + str(training) + ', testing: ' + str(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "logreg = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "logreg.fit(X_train,y_train)\n",
    "#print(logreg.score(X_test, y_test))\n",
    "print(classification_report(y_test, logreg.predict(X_test)))\n",
    "#print(confusion_matrix(y_test, logreg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
